{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjvQVbUbxmc567stRf+wRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-3/blob/Super_Murray/supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "zwkkSetusdVb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Reshape\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import nltk\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjngr6X6sk7r",
        "outputId": "336f5799-0d6a-4b0b-cf81-cbd0d32137fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_to_sents():\n",
        "    all_data = []\n",
        "    with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/train.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            word = line[0]\n",
        "            graphemes = line[1].split('-')\n",
        "\n",
        "            cur_word = []\n",
        "\n",
        "            for i in range(0, len(graphemes)):\n",
        "                for j in range(0, len(graphemes[i])):\n",
        "                    if j == 0:\n",
        "                        cur_word.append((graphemes[i][j], 'B'))\n",
        "                    else:\n",
        "                        cur_word.append((graphemes[i][j], 'I'))\n",
        "\n",
        "            all_data.append(cur_word)\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "pEpjh6qgsoYf"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(data):\n",
        "  vocab = list(set([w for sent in data for (w,t) in sent]))\n",
        "  vocab.append('<PAD>')\n",
        "  tags = list(set([t for sent in data for (w,t) in sent]))\n",
        "  tags.append('<PAD>')\n",
        "\n",
        "  return vocab, tags"
      ],
      "metadata": {
        "id": "SyCX8WWu4Wis"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(vocab, tags, data, load=False):\n",
        "  max_len = max([len(i) for i in data])\n",
        "\n",
        "  word2index = {}\n",
        "  tag2index = {}\n",
        "\n",
        "  if load is False:\n",
        "      word2index = {w: i for i, w in enumerate(vocab)}\n",
        "      tag2index = {t: i for i, t in enumerate(tags)}\n",
        "  else:\n",
        "      with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2index.json\") as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "      with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2index.json\") as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "  print(type(word2index))\n",
        "  print(type(tag2index))\n",
        "\n",
        "  onehot = [[word2index[w[0]] for w in s] for s in data]\n",
        "  X = pad_sequences(maxlen=max_len, sequences=onehot, padding=\"post\", value=len(vocab)-1)  \n",
        "\n",
        "  onehot_y = [[tag2index[w[1]] for w in s] for s in data]\n",
        "  y = pad_sequences(maxlen=max_len, sequences=onehot_y, padding=\"post\", value=tag2index[\"<PAD>\"])\n",
        "  y = to_categorical(y, num_classes=len(tags))\n",
        "\n",
        "  # Used for saving word2index and tag2index in order to encode additional data in the same manner\n",
        "  # Currently commented out due to issues with loading model\n",
        "  with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2index.json\", \"w\") as outfile:\n",
        "    json.dump(word2index, outfile)\n",
        "\n",
        "  with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2index.json\", \"w\") as outfile:\n",
        "    json.dump(tag2index, outfile)\n",
        "\n",
        "  return X, y, max_len"
      ],
      "metadata": {
        "id": "nzxivxza4ZKR"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_model(data):\n",
        "  #Original\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "  \n",
        "  \n",
        "  # Dr. Scannell\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(vocab), output_dim=50, input_length=max_len))\n",
        "    model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.05)))\n",
        "    model.add(TimeDistributed(Dense(len(tags), activation=\"softmax\")))\n",
        "    model.compile(optimizer=\"adam\", loss=\"poisson\", metrics=[\"accuracy\"])\n",
        "  # From https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "\n",
        "    print(\"[INFO] training network...\")\n",
        "    sgd = SGD(0.05)\n",
        "    history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.15, verbose=1, callbacks=stop_early)\n",
        "\n",
        "    return model, X_test, y_test"
      ],
      "metadata": {
        "id": "FVR17k8e4bxB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_precision(preds, y_test):\n",
        "    true_pos = 0\n",
        "    false_pos = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        if str(preds[i]) == 'B' and str(y_test[i]) == 'B':\n",
        "            true_pos += 1\n",
        "        if str(preds[i]) == 'B' and str(y_test[i]) == 'I':\n",
        "            false_pos += 1\n",
        "\n",
        "    if (true_pos + false_pos) == 0:\n",
        "        return 0.01\n",
        "\n",
        "    precision = 100 * (true_pos / (true_pos + false_pos))\n",
        "\n",
        "    return precision"
      ],
      "metadata": {
        "id": "Y9m8uO1d-_KH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_recall(preds, y_test):\n",
        "    true_pos = 0\n",
        "    false_neg = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        if str(preds[i]) == 'B' and str(y_test[i]) == 'B':\n",
        "            true_pos += 1\n",
        "        if str(preds[i]) == 'I' and str(y_test[i]) == 'B':\n",
        "            false_neg += 1\n",
        "\n",
        "    if true_pos + false_neg == 0:\n",
        "        return 0\n",
        "        \n",
        "    recall = 100 * (true_pos / (true_pos + false_neg))\n",
        "\n",
        "    return recall"
      ],
      "metadata": {
        "id": "KYP5sEdp_eg7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, x_test, y_test):\n",
        "    eval = model.evaluate(x_test, y_test)\n",
        "    print(eval)"
      ],
      "metadata": {
        "id": "k_MUYsLr40jx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def supervised():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    model, x_test, y_test = seq_model(data)\n",
        "\n",
        "    eval_model(model, x_test, y_test) # Eval sequential model\n",
        "\n",
        "    model.save('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32TEST_seq_model')\n",
        "\n",
        "    new_model = load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32TEST_seq_model')\n",
        "\n",
        "    eval_model(new_model, x_test, y_test)"
      ],
      "metadata": {
        "id": "-NtZSkDetvsB"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_eval_model():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data, load=True)\n",
        "\n",
        "    undo_encode(x, y)\n",
        "\n",
        "    new_model = load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32TEST_seq_model')\n",
        "\n",
        "    eval_model(new_model, x, y)"
      ],
      "metadata": {
        "id": "hvLR537CzpQg"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_encode(x, y):\n",
        "    all_data = []\n",
        "\n",
        "    word2index = {}\n",
        "    tag2index = {}\n",
        "\n",
        "    with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2index.json\") as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "    with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2index.json\") as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "    for word in x:\n",
        "        cur_word = []\n",
        "        for letter in word:\n",
        "            if letter == 51:\n",
        "                break\n",
        "            true_letter = list(word2index.keys())[list(word2index.values()).index(letter)]\n",
        "            cur_word.append(true_letter)\n",
        "\n",
        "        all_data.append(''.join(cur_word))\n",
        "\n",
        "    print(all_data[:5])"
      ],
      "metadata": {
        "id": "JvMIudG_2Zv-"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_eval_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "jJslV8Ryz3R2",
        "outputId": "7e022f21-c4ef-4e6f-9846-ff0e1591ff08"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 12812it [00:00, 152955.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "<class 'dict'>\n",
            "['konsiltan', 'depotwa', 'sosyopwofesyon√®l', 'vejetal', 'repibliye']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-92de7fb4b44b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_and_eval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-174-002b394d0638>\u001b[0m in \u001b[0;36mload_and_eval_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mundo_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32TEST_seq_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mnodes_to_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   loaded = tf.__internal__.saved_model.load_partial(\n\u001b[0;32m--> 142\u001b[0;31m       path, nodes_to_load, options=options)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;31m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mnode\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mto\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m   \"\"\"\n\u001b[0;32m--> 842\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m--> 975\u001b[0;31m                             ckpt_options, options, filters)\n\u001b[0m\u001b[1;32m    976\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mlibrary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0msaved_object_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             wrapper_function=_WrapperFunction))\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;31m# Store a set of all concrete functions that have been set up with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# captures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"_input_shapes\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_input_shapes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcreteFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrapper_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m       \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[0;31m# FuncGraph directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0;32m-> 1495\u001b[0;31m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[1;32m   1496\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[1;32m    584\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[1;32m    585\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[1;32m    587\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# control_output_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         compat.as_str(\"\"))\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supervised()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn5J86L8uH9u",
        "outputId": "119e229d-10bf-42d6-a380-f8060fe817b5"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 12812it [00:00, 176605.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/50\n",
            "307/307 [==============================] - 28s 70ms/step - loss: 0.3855 - accuracy: 0.9480 - val_loss: 0.3495 - val_accuracy: 0.9857\n",
            "Epoch 2/50\n",
            "307/307 [==============================] - 22s 73ms/step - loss: 0.3446 - accuracy: 0.9882 - val_loss: 0.3414 - val_accuracy: 0.9904\n",
            "Epoch 3/50\n",
            "307/307 [==============================] - 22s 71ms/step - loss: 0.3386 - accuracy: 0.9941 - val_loss: 0.3374 - val_accuracy: 0.9952\n",
            "Epoch 4/50\n",
            "307/307 [==============================] - 22s 71ms/step - loss: 0.3368 - accuracy: 0.9963 - val_loss: 0.3366 - val_accuracy: 0.9964\n",
            "Epoch 5/50\n",
            "307/307 [==============================] - 22s 71ms/step - loss: 0.3360 - accuracy: 0.9973 - val_loss: 0.3358 - val_accuracy: 0.9971\n",
            "Epoch 6/50\n",
            "307/307 [==============================] - 21s 70ms/step - loss: 0.3355 - accuracy: 0.9978 - val_loss: 0.3354 - val_accuracy: 0.9978\n",
            "Epoch 7/50\n",
            "307/307 [==============================] - 21s 68ms/step - loss: 0.3352 - accuracy: 0.9981 - val_loss: 0.3351 - val_accuracy: 0.9983\n",
            "Epoch 8/50\n",
            "307/307 [==============================] - 21s 68ms/step - loss: 0.3349 - accuracy: 0.9984 - val_loss: 0.3350 - val_accuracy: 0.9982\n",
            "Epoch 9/50\n",
            "307/307 [==============================] - 21s 70ms/step - loss: 0.3348 - accuracy: 0.9985 - val_loss: 0.3350 - val_accuracy: 0.9983\n",
            "Epoch 10/50\n",
            "307/307 [==============================] - 21s 69ms/step - loss: 0.3346 - accuracy: 0.9986 - val_loss: 0.3347 - val_accuracy: 0.9986\n",
            "Epoch 11/50\n",
            "307/307 [==============================] - 21s 70ms/step - loss: 0.3346 - accuracy: 0.9986 - val_loss: 0.3346 - val_accuracy: 0.9987\n",
            "Epoch 12/50\n",
            "307/307 [==============================] - 21s 68ms/step - loss: 0.3344 - accuracy: 0.9987 - val_loss: 0.3344 - val_accuracy: 0.9987\n",
            "Epoch 13/50\n",
            "307/307 [==============================] - 21s 67ms/step - loss: 0.3344 - accuracy: 0.9988 - val_loss: 0.3345 - val_accuracy: 0.9985\n",
            "Epoch 14/50\n",
            "307/307 [==============================] - 22s 70ms/step - loss: 0.3343 - accuracy: 0.9988 - val_loss: 0.3346 - val_accuracy: 0.9984\n",
            "Epoch 15/50\n",
            "307/307 [==============================] - 21s 69ms/step - loss: 0.3343 - accuracy: 0.9989 - val_loss: 0.3345 - val_accuracy: 0.9983\n",
            "Epoch 16/50\n",
            "307/307 [==============================] - 21s 68ms/step - loss: 0.3342 - accuracy: 0.9990 - val_loss: 0.3345 - val_accuracy: 0.9986\n",
            "41/41 [==============================] - 0s 11ms/step - loss: 0.3347 - accuracy: 0.9984\n",
            "[0.3346741795539856, 0.9983532428741455]\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f6ba6350390> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f6ba634b050> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 1s 13ms/step - loss: 0.3347 - accuracy: 0.9984\n",
            "[0.3346741795539856, 0.9983532428741455]\n"
          ]
        }
      ]
    }
  ]
}