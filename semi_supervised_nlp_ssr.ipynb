{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkQsCAhJ8GAQ4MLqbplhfT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psriraj17/NLP-3/blob/main/semi_supervised_nlp_ssr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The entire code was extracted from \"https://github.com/gregfromstl/word_segmentation/blob/master/semi-supervised.ipynb\""
      ],
      "metadata": {
        "id": "tbMZFxoFBdHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDCbnGTE297V",
        "outputId": "383998f7-4341-4572-b6dd-463aaae97b5f"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from math import log\n",
        "import numpy as np\n",
        "torch.set_printoptions(precision=10)\n",
        "\n",
        "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Rx8EpgdX_wQZ"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "dxYt-iXVpq_h"
      },
      "outputs": [],
      "source": [
        "train_file = '/content/drive/MyDrive/train_semi.tsv'\n",
        "val_split = 0.95\n",
        "\n",
        "states = {\n",
        "    'B' :0,\n",
        "    'I' :1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file):\n",
        "  print(\"Loading data from file {}...\".format(file))\n",
        "  file = open(file, 'r')\n",
        "  data = []\n",
        "  for line in file:\n",
        "      pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "      data.append(pieces)\n",
        "  print(\"Loaded {} sentences\".format(len(data)))\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "gw9_vtz13gfk"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = list(load_data(train_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE5tGBMn38kB",
        "outputId": "5bc21e89-b9b0-4317-b5ba-d31f56a65ebd"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from file /content/drive/MyDrive/train_semi.tsv...\n",
            "Loaded 12812 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Splitting data...\")\n",
        "num_train_samples = int(len(train_data)*(1-val_split))\n",
        "val_data = train_data[num_train_samples:]\n",
        "print(len(val_data),'validation set')\n",
        "train_data = train_data[:num_train_samples]\n",
        "print(len(train_data),\"training set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ov3gpd86FiO",
        "outputId": "c9266de9-75ad-47f9-810f-0bb4a509c2b3"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data...\n",
            "12172 validation set\n",
            "640 training set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIZJMjlk9iSd",
        "outputId": "2ea46e5d-ffcf-4f8a-f96d-ca37d63e1893"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['konsiltan', 'k-on-s-i-l-t-an']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_probabilities_from_counts(counts_dict):\n",
        "    counts_sum = sum(counts_dict.values())\n",
        "    probabilities_dict = {}\n",
        "    for count_id in counts_dict:\n",
        "        count = counts_dict[count_id]\n",
        "        probabilities_dict[count_id] = count / counts_sum\n",
        "    assert round(sum(probabilities_dict.values()), 2) == 1.0, \"All probabilities should sum to 1 but got {}\".format(round(sum(probabilities_dict.values()), 2))\n",
        "    return probabilities_dict"
      ],
      "metadata": {
        "id": "05CPgjIC9nV5"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def key_with_max_val(d):\n",
        "    \"\"\"https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\"\"\"\n",
        "    v = list(d.values())\n",
        "    k = list(d.keys())\n",
        "    return k[v.index(max(v))]"
      ],
      "metadata": {
        "id": "IhEaGKa0-ean"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_initial_state_probabilities(data):\n",
        "    initial_state_counts = states.copy()\n",
        "    # equal probability of starting\n",
        "    for state in initial_state_counts:\n",
        "        initial_state_counts[state] += 1\n",
        "    initial_state_probabilities = compute_probabilities_from_counts(initial_state_counts)\n",
        "    return initial_state_probabilities"
      ],
      "metadata": {
        "id": "ejtR8t-4-lPl"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_transition_state_probabilities(data):\n",
        "    # create a dictionary with two levels, the first being the previous state and the second being the current state\n",
        "    transition_state_counts = {state: states.copy() for state in states}\n",
        "    # since we enumerate over a list that excludes the first item, the enumeration index is one behind\n",
        "    for prev_idx, word in enumerate(data[1:]):\n",
        "        prev_state = data[prev_idx][1]\n",
        "        current_state = word[1]\n",
        "        if prev_state in transition_state_counts and current_state in transition_state_counts[prev_state]:\n",
        "            transition_state_counts[prev_state][current_state] += 1\n",
        "    # setting STOP count to 1 for all states to avoid zeros\n",
        "    for state in transition_state_counts:\n",
        "        transition_state_counts[state]['STOP'] = 1\n",
        "    transition_state_probabilities = {state: {} for state in states}\n",
        "    for prev_state in transition_state_counts:\n",
        "        transition_state_probabilities[prev_state] = compute_probabilities_from_counts(transition_state_counts[prev_state])\n",
        "    return transition_state_probabilities"
      ],
      "metadata": {
        "id": "bnEzkkf0-08k"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_emission_probabilities(data, all_observations):\n",
        "    vocab = {obs: 1 for obs in set(all_observations)}\n",
        "    emission_counts_by_state = {state: vocab for state in states}\n",
        "    for word_state_pair in data:\n",
        "        word, state = word_state_pair\n",
        "        if state in emission_counts_by_state:\n",
        "            # initialize word in state dict if the first occurrence of word X in state Y\n",
        "            if word not in emission_counts_by_state[state]:\n",
        "                emission_counts_by_state[state][word] = 0\n",
        "            emission_counts_by_state[state][word] += 1\n",
        "    emission_probabilities_by_state = {state: {} for state in states}\n",
        "    for state in emission_counts_by_state:\n",
        "        emission_probabilities_by_state[state] = compute_probabilities_from_counts(emission_counts_by_state[state])\n",
        "    return emission_probabilities_by_state"
      ],
      "metadata": {
        "id": "OQ6Lyoet_C-g"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(data, vocab):\n",
        "  print(\"Fitting model to provided dataset...\")\n",
        "  initial_state_probabilities = generate_initial_state_probabilities(data)\n",
        "  transition_probabilities = generate_transition_state_probabilities(data)\n",
        "  emission_probabilities = generate_emission_probabilities(data, vocab)\n",
        "  print(\"Model ready.\")\n",
        "  return initial_state_probabilities, transition_probabilities, emission_probabilities"
      ],
      "metadata": {
        "id": "Gqb77Vv6_SvG"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_sequence = [pair[0] for pair in val_data]\n",
        "val_labels = [pair[1] for pair in val_data]\n",
        "initial, transition, emission  = fit(train_data, val_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rudivogO_YO5",
        "outputId": "c2af030f-eab1-4734-8ea5-1d36fd690e0d"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting model to provided dataset...\n",
            "Model ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_forward_prob(observations):\n",
        "    forward_probabilities = {}\n",
        "    for idx, observation in enumerate(observations):\n",
        "        for state in initial:\n",
        "            if idx == 0:\n",
        "                probability = initial[state]*emission[state][observation] if observation in emission[state] else 0\n",
        "                forward_probabilities[state] = torch.DoubleTensor([probability])\n",
        "            else:\n",
        "                probability = 0\n",
        "                for prev_state in transition:\n",
        "                    probability += forward_probabilities[prev_state][idx-1]*transition[prev_state][state]\n",
        "                probability *= emission[state][observation]\n",
        "                forward_probabilities[state] = torch.cat((forward_probabilities[state], torch.DoubleTensor([probability])))\n",
        "    return forward_probabilities"
      ],
      "metadata": {
        "id": "tREcVHI3_orQ"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_backward_prob(observations):\n",
        "    backward_probabilities = {}\n",
        "    for i in range(len(observations)):\n",
        "        idx = len(observations)-i-1\n",
        "        for state in initial:\n",
        "            if idx == len(observations)-1:\n",
        "                probability = transition[state]['STOP']\n",
        "                backward_probabilities[state] = torch.DoubleTensor([probability])\n",
        "            else:\n",
        "                probability = 0\n",
        "                for next_state in transition:\n",
        "                    probability += backward_probabilities[next_state][0]*transition[state][next_state]*emission[next_state][observations[idx+1]]\n",
        "                backward_probabilities[state] = torch.cat((torch.DoubleTensor([probability]), backward_probabilities[state]))\n",
        "    return backward_probabilities"
      ],
      "metadata": {
        "id": "mTNpYSgRqBQC"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_state_total_prob(forward_prob: dict, backward_prob: dict) -> dict:\n",
        "    state_total_prob: dict = {}\n",
        "    for state in forward_prob:\n",
        "        state_total_prob[state] = forward_prob[state] * backward_prob[state]\n",
        "    return state_total_prob"
      ],
      "metadata": {
        "id": "_egcAV72qgBX"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_total_prob(state_total_prob: dict, observations: list) -> torch.DoubleTensor:\n",
        "    combined_state_totals = [state_total_prob[state] for state in state_total_prob]\n",
        "    return torch.stack(combined_state_totals, dim=0).sum(dim=0)"
      ],
      "metadata": {
        "id": "F2U9HMIOqkFi"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_state_prob(state_total_prob: dict, total_prob: torch.DoubleTensor) -> dict:\n",
        "    new_state_prob: dict = {}\n",
        "    for state in state_total_prob:\n",
        "        new_state_prob[state] = state_total_prob[state] / total_prob\n",
        "    return new_state_prob"
      ],
      "metadata": {
        "id": "FsGWmMxyqlt9"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_observation_state_prob(new_state_prob: dict, observations: list) -> dict:\n",
        "    observation_state_prob: dict = {}\n",
        "    for state in new_state_prob:\n",
        "        observation_state_prob[state] = {}\n",
        "        for tag in set(observations):\n",
        "            observation_state_prob[state][tag] = torch.DoubleTensor([])\n",
        "            for idx, observation in enumerate(observations):\n",
        "                probability = new_state_prob[state][idx] if observation == tag else 0\n",
        "                observation_state_prob[state][tag] = torch.cat((observation_state_prob[state][tag], torch.DoubleTensor([probability])))\n",
        "    return observation_state_prob"
      ],
      "metadata": {
        "id": "dbUcRAt5qnsx"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transition_state_prob(forward_prob: dict, backward_prob: dict, total_prob: torch.DoubleTensor, observations: list) -> dict:\n",
        "    transition_state_prob: dict = {}\n",
        "    for prev_state in emission:\n",
        "        transition_state_prob[prev_state] = {}\n",
        "        for state in emission:\n",
        "            transition_state_prob[prev_state][state] = torch.DoubleTensor([])\n",
        "            for i, observation in enumerate(observations[1:]):\n",
        "                idx = i + 1\n",
        "                probability = forward_prob[prev_state][idx-1]*backward_prob[state][idx]\n",
        "                probability *= transition[prev_state][state]*emission[state][observation]\n",
        "                probability /= total_prob[idx]\n",
        "                transition_state_prob[prev_state][state] = torch.cat((transition_state_prob[prev_state][state], torch.DoubleTensor([probability])))\n",
        "    return transition_state_prob"
      ],
      "metadata": {
        "id": "1eXfTBFLqpou"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_emissions(observation_state_prob: dict, new_state_prob: dict) -> dict:\n",
        "    emissions = {}\n",
        "    for state in new_state_prob:\n",
        "        emissions[state] = {}\n",
        "        for tag in observation_state_prob[state]:\n",
        "            emissions[state][tag] = torch.sum(observation_state_prob[state][tag])/torch.sum(new_state_prob[state])\n",
        "    return emissions"
      ],
      "metadata": {
        "id": "dMFBksexqpfW"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transitions(transition_state_prob: dict, new_state_prob: dict) -> dict:\n",
        "    transitions = {}\n",
        "    for prev_state in transition_state_prob:\n",
        "        transitions[prev_state] = {}\n",
        "        for state in transition_state_prob:\n",
        "            transitions[prev_state][state] = torch.sum(transition_state_prob[prev_state][state])/torch.sum(new_state_prob[state])\n",
        "        transitions[prev_state]['STOP'] = new_state_prob[prev_state][-1]/torch.sum(new_state_prob[prev_state])\n",
        "    return transitions"
      ],
      "metadata": {
        "id": "aYVxif4kqsDq"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_init(new_state_prob: dict) -> dict:\n",
        "    init = {}\n",
        "    for state in new_state_prob:\n",
        "        init[state] = new_state_prob[state][0]\n",
        "    return init"
      ],
      "metadata": {
        "id": "YKb6zKwXqsBE"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate(observations: list):\n",
        "    forward: dict = get_forward_prob(observations)\n",
        "    backward: dict = get_backward_prob(observations)\n",
        "    state_total: dict = get_state_total_prob(forward, backward)\n",
        "    total: torch.DoubleTensor = get_total_prob(state_total, observations)\n",
        "    new_state: dict = get_new_state_prob(state_total, total)\n",
        "    observation_state: dict = get_observation_state_prob(new_state, observations)\n",
        "    transition_state: dict = get_transition_state_prob(forward, backward, total, observations)\n",
        "    emissions = get_emissions(observation_state, new_state)\n",
        "    transitions = get_transitions(transition_state, new_state)\n",
        "    init = get_init(new_state)\n",
        "    return init, transitions, emissions"
      ],
      "metadata": {
        "id": "eld2cx1Sqr-N"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences: list = []\n",
        "sequence_labels: list = []\n",
        "current: list = []\n",
        "current_labels: list = []\n",
        "sequence_size: int = 10\n",
        "print(\"Dividing unsupervised data...\")\n",
        "for idx, character in enumerate(val_sequence):\n",
        "    current.append(character)\n",
        "    current_labels.append(val_labels[idx])\n",
        "    if idx % sequence_size == 0 and idx > 0:\n",
        "        sequences.append(current)\n",
        "        sequence_labels.append(current_labels)\n",
        "        current = []\n",
        "        current_labels = []\n",
        "print(\"{} sequences of unsupervised data of length {}\".format(len(sequences), sequence_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcwKTKm8qr4v",
        "outputId": "38f243ee-6012-4dbf-e0e2-fd59ec291c1d"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dividing unsupervised data...\n",
            "1217 sequences of unsupervised data of length 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def has_nans(d: dict) -> bool:\n",
        "    for i in d.values():\n",
        "        if isinstance(i,dict):\n",
        "            if has_nans(i):\n",
        "                return True\n",
        "        else:\n",
        "            if i != i:\n",
        "                return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "vIgegWtHqzRY"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fitting...\")\n",
        "for sequence in sequences[:200000]:\n",
        "    try:\n",
        "        batch_initials, batch_transitions, batch_emissions = iterate(sequence)\n",
        "        transition = batch_transitions if not has_nans(batch_transitions) else transition\n",
        "        if not has_nans(batch_emissions):\n",
        "            for state in emission:\n",
        "                emission[state].update(batch_emissions)\n",
        "    except:\n",
        "        pass\n",
        "print(\"Generated probabilities based on unlabeled data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNiE0nZKqzPj",
        "outputId": "91161929-8502-4ad0-c558-ed827208aa61"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting...\n",
            "Generated probabilities based on unlabeled data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, state: str, probability: float, back_pointer):\n",
        "        self.back_pointer: Node = back_pointer\n",
        "        self.state: str = state\n",
        "        self.probability: float = probability\n",
        "\n",
        "\n",
        "def keys_match(dict_a: dict, dict_b: dict) -> bool:\n",
        "    return dict_a.keys() == dict_b.keys()\n",
        "\n",
        "\n",
        "def key_with_max_val(d: dict) -> str:\n",
        "    \"\"\"https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\"\"\"\n",
        "    v = list(d.values())\n",
        "    k = list(d.keys())\n",
        "    return k[v.index(max(v))]\n",
        "\n",
        "\n",
        "def node_with_max_prob(d: dict) -> Node:\n",
        "    max_node = Node(None, 0.0, None)\n",
        "    for node in d.values():\n",
        "        if node.probability > max_node.probability:\n",
        "            max_node = node\n",
        "    return max_node\n",
        "\n",
        "\n",
        "class Viterbi:\n",
        "    def __init__(self, initial_probabilities: dict, transition_probabilities: dict, emission_probabilities: dict,):\n",
        "        assert keys_match(initial_probabilities, emission_probabilities) and\\\n",
        "               keys_match(initial_probabilities, transition_probabilities), \"Hidden states must be consistent!\"\n",
        "        self.initial = initial_probabilities\n",
        "        self.emission = emission_probabilities\n",
        "        self.transitions = transition_probabilities\n",
        "\n",
        "    def predict_path(self, observations: list) -> list:\n",
        "        matrix: list = [{}]\n",
        "\n",
        "        for state in self.initial:\n",
        "            matrix[0][state] = Node(state, self.initial[state]*self.emission[state][observations[0]], None)\n",
        "\n",
        "        # fill initial probabilities\n",
        "        for prev_idx, observation in enumerate(observations[1:]):\n",
        "            matrix.append({})\n",
        "            for state in self.transitions:\n",
        "                transitions: dict = {}\n",
        "                for prev_state in matrix[prev_idx]:\n",
        "                    prev_prob = matrix[prev_idx][prev_state].probability\n",
        "                    transition_prob = self.transitions[prev_state][state]*prev_prob\n",
        "                    transitions[prev_state] = transition_prob\n",
        "                last_state = key_with_max_val(transitions)\n",
        "                probability = self.emission[state][observation]*transitions[last_state]\n",
        "                matrix[prev_idx+1][state] = Node(state, probability, matrix[prev_idx][last_state])\n",
        "\n",
        "        current_node: Node = node_with_max_prob(matrix[-1])\n",
        "        sequence: list = []\n",
        "        while current_node is not None:\n",
        "            sequence.insert(0, current_node.state)\n",
        "            current_node = current_node.back_pointer\n",
        "\n",
        "        return sequence"
      ],
      "metadata": {
        "id": "gGo-2zoQqzM9"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viterbi = Viterbi(initial, transition, emission)"
      ],
      "metadata": {
        "id": "eT6gF2ZnqzKG"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(preds: list, labels: list) -> float:\n",
        "    corrects: list = []\n",
        "    for idx, pred in enumerate(preds):\n",
        "        corrects.append(pred == labels[idx])\n",
        "    return sum(corrects) / len(corrects)"
      ],
      "metadata": {
        "id": "EZi82M8aqzHP"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_precision(preds, y):\n",
        "    true_positive_preds = 0\n",
        "    positive_preds = 0\n",
        "    for idx, pred in enumerate(preds):\n",
        "        if pred == '1':\n",
        "            positive_preds += 1\n",
        "            if y[idx] == '1':\n",
        "                true_positive_preds += 1\n",
        "    return true_positive_preds / positive_preds"
      ],
      "metadata": {
        "id": "6I9eaJJYq89b"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recall(preds, y):\n",
        "    true_positive_preds = 0\n",
        "    positive_labels = 0\n",
        "    for idx, label in enumerate(y):\n",
        "        if label == '1':\n",
        "            positive_labels += 1\n",
        "            if preds[idx] == '1':\n",
        "                true_positive_preds += 1\n",
        "    return true_positive_preds / positive_labels"
      ],
      "metadata": {
        "id": "BfXlowsJq87F"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1score(precision, recall):\n",
        "    return 2*((precision*recall)/(precision+recall))"
      ],
      "metadata": {
        "id": "HCyV4zatq84f"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "precision = 0\n",
        "recall = 0\n",
        "idx = 0\n",
        "print(\"Evaluating...\")\n",
        "for sequence in sequences:\n",
        "    try:\n",
        "        predictions = viterbi.predict_path(sequence)\n",
        "        labels = sequence_labels[idx]\n",
        "        a = get_accuracy(predictions, labels)\n",
        "        p = get_precision(predictions, labels)\n",
        "        r = get_recall(predictions, labels)\n",
        "        # make sure all functions succeed, then accumulate\n",
        "        acc += a\n",
        "        precision += p\n",
        "        recall += r\n",
        "        idx += 1\n",
        "    except:\n",
        "        pass\n",
        "precision = precision / (idx+1)\n",
        "recall = recall / (idx+1)\n",
        "print(\"Validation Metrics:\")\n",
        "print(\"\\tAccuracy: \", \"{:0.2f}%\".format(acc / (idx+1) * 100))\n",
        "print(\"\\tPrecision: \", \"{:0.2f}\".format(precision))\n",
        "print(\"\\tRecall: \", \"{:0.2f}\".format(recall))\n",
        "print(\"\\tF1 Score: \", \"{:0.2f}\".format(f1score(precision, recall)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "nE8FQ_BQrHxX",
        "outputId": "7397ff3e-067d-48de-bd78-8d43feb17d38"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n",
            "Validation Metrics:\n",
            "\tAccuracy:  0.00%\n",
            "\tPrecision:  0.00\n",
            "\tRecall:  0.00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-7be66ad068b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tPrecision: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:0.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tRecall: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:0.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tF1 Score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:0.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-145-07dae7f426a3>\u001b[0m in \u001b[0;36mf1score\u001b[0;34m(precision, recall)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf1score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"test_data\" in globals():\n",
        "    test_sequence = [pair[0] for pair in test_data]\n",
        "    test_labels = [pair[1] for pair in test_data]\n",
        "    sequences: list = []\n",
        "    sequence_labels: list = []\n",
        "    current: list = []\n",
        "    current_labels: list = []\n",
        "    sequence_size: int = 10\n",
        "    print(\"Dividing test data...\")\n",
        "    for idx, character in enumerate(test_sequence):\n",
        "        current.append(character)\n",
        "        current_labels.append(test_labels[idx])\n",
        "        if idx % sequence_size == 0 and idx > 0:\n",
        "            sequences.append(current)\n",
        "            sequence_labels.append(current_labels)\n",
        "            current = []\n",
        "            current_labels = []\n",
        "    print(\"{} sequences of test data of length {}\".format(len(sequences), sequence_size))\n",
        "    \n",
        "    acc = 0\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    idx = 0\n",
        "    print(\"Evaluating...\")\n",
        "    for sequence in sequences:\n",
        "        try:\n",
        "            predictions = viterbi.predict_path(sequence)\n",
        "            labels = sequence_labels[idx]\n",
        "            a = get_accuracy(predictions, labels)\n",
        "            p = get_precision(predictions, labels)\n",
        "            r = get_recall(predictions, labels)\n",
        "            # make sure all functions succeed, then accumulate\n",
        "            acc += a\n",
        "            precision += p\n",
        "            recall += r\n",
        "            idx += 1\n",
        "        except:\n",
        "            pass\n",
        "    precision = precision / (idx+1)\n",
        "    recall = recall / (idx+1)\n",
        "    print(\"Testing Metrics:\")\n",
        "    print(\"\\tAccuracy: \", \"{:0.2f}%\".format(acc / (idx+1) * 100))\n",
        "    print(\"\\tPrecision: \", \"{:0.2f}\".format(precision))\n",
        "    print(\"\\tRecall: \", \"{:0.2f}\".format(recall))\n",
        "    print(\"\\tF1 Score: \", \"{:0.2f}\".format(f1score(precision, recall)))"
      ],
      "metadata": {
        "id": "-cg-3s72rHuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}