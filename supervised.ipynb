{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUPBUCXZ0vNUFLjcqWNgAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-3/blob/Super_Murray/supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zwkkSetusdVb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Reshape\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import nltk\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "-pxCyfyNrbau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2indextest.json | /content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2indextest.json PAIR WITH LOW_DIM\n",
        "#/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2indexdectree.json | /content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2indextestdectree.json PAIR WITH DEC TREE\n",
        "\n",
        "\n",
        "word2index_file = \"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2indextestNEW.json\"\n",
        "tag2index_file = \"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2indextestNEW.json\""
      ],
      "metadata": {
        "id": "0kFOnQg0wQX-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjngr6X6sk7r",
        "outputId": "fdb882ab-c0cc-4085-fdc0-c19db4f5f4cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "LOrbZInp2lY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_to_sents():\n",
        "    all_data = []\n",
        "    with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/train.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            word = line[0]\n",
        "            graphemes = line[1].split('-')\n",
        "\n",
        "            cur_word = []\n",
        "\n",
        "            for i in range(0, len(graphemes)):\n",
        "                for j in range(0, len(graphemes[i])):\n",
        "                    if j == 0:\n",
        "                        cur_word.append((graphemes[i][j], 'B'))\n",
        "                    else:\n",
        "                        cur_word.append((graphemes[i][j], 'I'))\n",
        "\n",
        "            all_data.append(cur_word)\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "pEpjh6qgsoYf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_test_data():\n",
        "    all_data = []\n",
        "    with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/test.txt\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            word = line[0]\n",
        "\n",
        "            all_data.append([*word])\n",
        "\n",
        "    print(len(all_data))\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "hYH-4qy3HyVv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Challenge 2 https://github.com/GreihMurray/NLP-2"
      ],
      "metadata": {
        "id": "mgV1hD6F2nLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(data):\n",
        "  vocab = list(set([w for sent in data for (w,t) in sent]))\n",
        "  vocab.append('<PAD>')\n",
        "  tags = list(set([t for sent in data for (w,t) in sent]))\n",
        "  tags.append('<PAD>')\n",
        "\n",
        "  return vocab, tags"
      ],
      "metadata": {
        "id": "SyCX8WWu4Wis"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_test(data):\n",
        "    vocab = list(set([w for sent in data for w in sent]))\n",
        "    vocab.append('<PAD>')\n",
        "\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "jpY_FkTwH0OA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combination of code from Challenge 2 (https://github.com/GreihMurray/NLP-2) and custom"
      ],
      "metadata": {
        "id": "KBQf1D3z2uIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(vocab, tags, data, load=False):\n",
        "  max_len = max([len(i) for i in data])\n",
        "\n",
        "  word2index = {}\n",
        "  tag2index = {}\n",
        "\n",
        "  if load is False:\n",
        "      word2index = {w: i for i, w in enumerate(vocab)}\n",
        "      tag2index = {t: i for i, t in enumerate(tags)}\n",
        "  else:\n",
        "      with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "      with open(tag2index_file) as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "  onehot = [[word2index[w[0]] for w in s] for s in data]\n",
        "  X = pad_sequences(maxlen=max_len, sequences=onehot, padding=\"post\", value=len(vocab)-1)  \n",
        "\n",
        "  onehot_y = [[tag2index[w[1]] for w in s] for s in data]\n",
        "  y = pad_sequences(maxlen=max_len, sequences=onehot_y, padding=\"post\", value=tag2index[\"<PAD>\"])\n",
        "  y = to_categorical(y, num_classes=len(tags))\n",
        "\n",
        "  # Used for saving word2index and tag2index in order to encode additional data in the same manner\n",
        "  # Currently commented out due to issues with loading model\n",
        "  with open(word2index_file, \"w\") as outfile:\n",
        "    json.dump(word2index, outfile)\n",
        "\n",
        "  with open(tag2index_file, \"w\") as outfile:\n",
        "    json.dump(tag2index, outfile)\n",
        "\n",
        "  return X, y, max_len"
      ],
      "metadata": {
        "id": "nzxivxza4ZKR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_test(vocab, data, load=True):\n",
        "  max_len = 18\n",
        "\n",
        "  word2index = {}\n",
        "\n",
        "  if load is False:\n",
        "      word2index = {w: i for i, w in enumerate(vocab)}\n",
        "  else:\n",
        "      with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "  onehot = [[word2index[w[0]] for w in s] for s in data]\n",
        "  X = pad_sequences(maxlen=max_len, sequences=onehot, padding=\"post\", value=len(vocab))  \n",
        "\n",
        "  return X, max_len"
      ],
      "metadata": {
        "id": "gJijEiLhH8NZ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code from Challenge 2 & Dr. Scannell (https://github.com/GreihMurray/NLP-2, "
      ],
      "metadata": {
        "id": "J1ZWGtZD21XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_model(data):\n",
        "  #Original\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "  \n",
        "  \n",
        "  # Dr. Scannell\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(vocab), output_dim=15, input_length=max_len))\n",
        "    model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.01)))\n",
        "    model.add(TimeDistributed(Dense(len(tags), activation=\"softmax\")))\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "  # From https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "\n",
        "    print(\"[INFO] training network...\")\n",
        "    sgd = SGD(0.05)\n",
        "    history = model.fit(X_train, y_train, batch_size=16, epochs=50, validation_split=0.15, verbose=1, callbacks=stop_early)\n",
        "\n",
        "    return model, X_test, y_test"
      ],
      "metadata": {
        "id": "FVR17k8e4bxB"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "68ZgdUZK3ABg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, x_test, y_test):\n",
        "    eval = model.evaluate(x_test, y_test)\n",
        "    print(eval)"
      ],
      "metadata": {
        "id": "k_MUYsLr40jx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "rZJnczhe3CiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def supervised():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    print(data[:5])\n",
        "\n",
        "    model, x_test, y_test = seq_model(data)\n",
        "\n",
        "    eval_model(model, x_test, y_test) # Eval sequential model\n",
        "\n",
        "    model.save('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32_seq_model_low_dimTESTNEW') # Current best adamPoisson32_seq_model_low_dim\n",
        "\n",
        "    new_model = load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32_seq_model_low_dimTESTNEW')\n",
        "\n",
        "    custom_eval(new_model, x_test, y_test)"
      ],
      "metadata": {
        "id": "-NtZSkDetvsB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "c79gNMAf3EBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_eval_model():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data, load=True)\n",
        "\n",
        "    new_model = load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32_seq_model_low_dim')\n",
        "\n",
        "    evals = new_model.evaluate(x, y)\n",
        "\n",
        "    print('Default Accuracy: ', (evals[1] * 100))\n",
        "\n",
        "    preds = custom_eval(new_model, x, y)\n",
        "    clean_x = undo_encode_x(x)\n",
        "    combined = recombine(clean_x, preds)\n",
        "\n",
        "    graphs = to_graphemes(combined)\n",
        "\n",
        "    print_results_to_file(graphs)\n"
      ],
      "metadata": {
        "id": "hvLR537CzpQg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_eval_test():\n",
        "    print(\"Reading data\")\n",
        "    data = read_test_data()\n",
        "\n",
        "    print(\"Encoding data\")\n",
        "    vocab = pad_test(data)\n",
        "    x, _ = encode_test(vocab, data)\n",
        "\n",
        "    print(\"Loading model\")\n",
        "    model = load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32_seq_model_low_dimTESTNEW')\n",
        "\n",
        "    print(\"Predicting\")\n",
        "    preds = model.predict(x)\n",
        "\n",
        "    print(\"Preparing predictions\")\n",
        "    preds = undo_encode_y(preds)\n",
        "    clean_x = undo_encode_x(x)\n",
        "\n",
        "    print(x[25:30])\n",
        "    print(clean_x[25:30])\n",
        "\n",
        "    combined = recombine(clean_x, preds)\n",
        "    print(combined[:5])\n",
        "\n",
        "    print(\"Converting to graphemes\")\n",
        "    graphs = to_graphemes(combined)\n",
        "\n",
        "    print(\"Printing to file\")\n",
        "    print_results_to_file(graphs)"
      ],
      "metadata": {
        "id": "9QuxjXAoIHrl"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "DVkf7ZNIredZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results_to_file(data):\n",
        "    with open('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/supervised_resultsTEST.tsv', 'w', newline='') as tsvfile:\n",
        "      writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
        "\n",
        "      for row in data:\n",
        "          writer.writerow(row)"
      ],
      "metadata": {
        "id": "47wFNQAOqdOt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "huJK3T_T3FtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recombine(words, preds):\n",
        "    all_data = []\n",
        "\n",
        "    for i in range(0, len(words)):\n",
        "        all_data.append((words[i], preds[i]))\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "d6R7XviOmsFj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "v8UHkTSlrgq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_encode_x(x, test=False):\n",
        "    all_words = []\n",
        "\n",
        "    pad_val = 51\n",
        "\n",
        "    if test == True:\n",
        "        pad_val = 50\n",
        "\n",
        "    word2index = {}\n",
        "    tag2index = {}\n",
        "\n",
        "    with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "    with open(tag2index_file) as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "    for word in x:\n",
        "        cur_word = []\n",
        "        for letter in word:\n",
        "            if letter == pad_val:\n",
        "                break\n",
        "            true_letter = list(word2index.keys())[list(word2index.values()).index(letter)]\n",
        "            cur_word.append(true_letter)\n",
        "\n",
        "        all_words.append(''.join(cur_word))\n",
        "\n",
        "    return all_words"
      ],
      "metadata": {
        "id": "fZXYEXsZpZn2"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "NC7XKS-xriR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_encode(x, y):\n",
        "    all_data = []\n",
        "    all_words = []\n",
        "    all_tags = []\n",
        "\n",
        "    word2index = {}\n",
        "    tag2index = {}\n",
        "\n",
        "    with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "    with open(tag2index_file) as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "    for word in x:\n",
        "        cur_word = []\n",
        "        for letter in word:\n",
        "            if letter == 51:\n",
        "                break\n",
        "            true_letter = list(word2index.keys())[list(word2index.values()).index(letter)]\n",
        "            cur_word.append(true_letter)\n",
        "\n",
        "        all_words.append(''.join(cur_word))\n",
        "\n",
        "    for tags in y:\n",
        "        cur_tags = []\n",
        "        for tag in tags:\n",
        "            if tag[0] == 1:\n",
        "                cur_tags.append('I')\n",
        "            elif tag[1] == 1:\n",
        "                cur_tags.append('B')\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        all_tags.append(''.join(cur_tags))\n",
        "\n",
        "    for i in range(0, len(all_words)):\n",
        "        all_data.append((all_words[i], all_tags[i]))\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "JvMIudG_2Zv-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "Pf3yr4043Hs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_encode_y(y):\n",
        "    all_tags = []\n",
        "\n",
        "    word2index = {}\n",
        "    tag2index = {}\n",
        "\n",
        "    with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "    with open(tag2index_file) as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "    for tags in y:\n",
        "        cur_tags = []\n",
        "        for tag in tags:\n",
        "            if round(tag[0]) == 1:\n",
        "                cur_tags.append('B')\n",
        "            elif round(tag[1]) == 1:\n",
        "                cur_tags.append('I')\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        all_tags.append(''.join(cur_tags))\n",
        "\n",
        "    return all_tags"
      ],
      "metadata": {
        "id": "u3ZQii5qtmgl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "akUVMppX6Y82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(preds, y_test):\n",
        "    total_right = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        if preds[i] == y_test[i]:\n",
        "            total_right += 1\n",
        "\n",
        "    accuracy = 100 * (total_right/len(preds))\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "whY7VrS66Fzz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "TLJAlt3D3J6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_precision(preds, y_test):\n",
        "    true_pos = 0\n",
        "    false_pos = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        for j in range(0, len(preds[i])):\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'I':\n",
        "                true_pos += 1\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'B':\n",
        "                false_pos += 1\n",
        "\n",
        "    if (true_pos + false_pos) == 0:\n",
        "        return 0.01\n",
        "\n",
        "    precision = 100 * (true_pos / (true_pos + false_pos))\n",
        "\n",
        "    return precision"
      ],
      "metadata": {
        "id": "Y9m8uO1d-_KH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "L47S6Rz93Ldb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_recall(preds, y_test):\n",
        "    true_pos = 0\n",
        "    false_neg = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        for j in range(0, len(preds[i])):\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'I':\n",
        "                true_pos += 1\n",
        "            if preds[i][j] == 'B' and y_test[i][j] == 'I':\n",
        "                false_neg += 1\n",
        "\n",
        "    if true_pos + false_neg == 0:\n",
        "        return 0\n",
        "        \n",
        "    recall = 100 * (true_pos / (true_pos + false_neg))\n",
        "\n",
        "    return recall"
      ],
      "metadata": {
        "id": "KYP5sEdp_eg7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "5q2qzQH_3NqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_eval(model, x, y):\n",
        "    preds = model.predict(x)\n",
        "\n",
        "    preds = undo_encode_y(preds)\n",
        "    y_clean = undo_encode_y(y)\n",
        "\n",
        "    acc = calc_accuracy(preds, y_clean)\n",
        "\n",
        "    print(\"Custom calculated Accuracy: \", acc)\n",
        "\n",
        "    prec = calc_precision(preds, y_clean)\n",
        "\n",
        "    print(\"Precision: \", prec)\n",
        "\n",
        "    recall = calc_recall(preds, y_clean)\n",
        "\n",
        "    print(\"Recall: \", recall)\n",
        "\n",
        "    fscore = (2 * (prec * recall)) / (prec + recall)\n",
        "\n",
        "    print(\"Fscore: \", fscore)\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "fIO2cOGUs9Ma"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "XOuh_XOC4KGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dec_tree_eval(preds, y_test):\n",
        "    acc = calc_accuracy(preds, y_test)\n",
        "\n",
        "    print(\"Custom calculated Accuracy: \", acc)\n",
        "\n",
        "    prec = calc_precision(preds, y_test)\n",
        "\n",
        "    print(\"Precision: \", prec)\n",
        "\n",
        "    recall = calc_recall(preds, y_test)\n",
        "\n",
        "    print(\"Recall: \", recall)\n",
        "\n",
        "    fscore = (2 * (prec * recall)) / (prec + recall)\n",
        "\n",
        "    print(\"Fscore: \", fscore)"
      ],
      "metadata": {
        "id": "M7Nm9ZDiyaqy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "CwJvFEFG3PvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_graphemes(data):\n",
        "    graph_data = []\n",
        "\n",
        "    for word_pair in data:\n",
        "        word = word_pair[0]\n",
        "        grap = word_pair[1]\n",
        "\n",
        "        cur_word = []\n",
        "\n",
        "        for i in range(0, len(word)):\n",
        "            if i == (len(word) - 1):\n",
        "                cur_word.append(word[i])\n",
        "\n",
        "            else:\n",
        "                if grap[i+1] == 'I':\n",
        "                    cur_word.append(word[i])\n",
        "                else:\n",
        "                    cur_word.append(word[i] + '-')\n",
        "\n",
        "        graph_data.append((word, ''.join(cur_word)))\n",
        "\n",
        "    return graph_data\n"
      ],
      "metadata": {
        "id": "Q67tirhiFCSp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "ltdgLJ9d4LrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_x_y(data):\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "\n",
        "    for word in data:\n",
        "        cur_x = []\n",
        "        cur_y = []\n",
        "        for letter in word:\n",
        "            all_x.append(letter[0])\n",
        "            all_y.append(letter[1])\n",
        "        \n",
        "\n",
        "    return np.asarray(all_x), np.asarray(all_y)"
      ],
      "metadata": {
        "id": "SX-g9foVrqyU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostly original, some work from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "Qr4wSEaB4NIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dec_tree():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    x, y = split_x_y(data)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=50)\n",
        "\n",
        "    y_test_clean = y_test\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.fit(x_train)\n",
        "    x_train = le.transform(x_train)\n",
        "    x_train = x_train.reshape(-1, 1)\n",
        "    x_test = le.transform(x_test)\n",
        "    x_test = x_test.reshape(-1, 1)\n",
        "\n",
        "    yle = LabelEncoder()\n",
        "    yle.fit(y_train)\n",
        "    y_train = yle.transform(y_train)\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_test = yle.transform(y_test)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "    clf = Pipeline([\n",
        "        ('classifier', DecisionTreeClassifier(criterion='entropy')),\n",
        "    ], verbose=1)\n",
        "\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    preds = clf.predict(x_test)\n",
        "\n",
        "\n",
        "    clean_preds = yle.inverse_transform(preds)\n",
        "\n",
        "    dec_tree_eval(clean_preds, y_test_clean)"
      ],
      "metadata": {
        "id": "TL0UWmRnrm3q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "-xDs7QF44WZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hmm_eval(model, test, model_type):\n",
        "    acc = model.accuracy(test)\n",
        "\n",
        "    print(model_type, \" Accuracy: \", acc)\n",
        "\n",
        "    prec = model.precision(test)\n",
        "\n",
        "    print(model_type, ' Precision: ', prec)\n",
        "\n",
        "    rec = model.recall(test)\n",
        "\n",
        "    print(model_type, ' Recall: ', rec)\n",
        "\n",
        "    fscore = (2 * (prec['I'] * rec['I'])) / (prec['I'] + rec['I'])\n",
        "\n",
        "    print(model_type, ' FScore (I): ', fscore)\n",
        "\n",
        "    print('\\n\\n')"
      ],
      "metadata": {
        "id": "8JFxCMl52fp5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "-MhxQZGT4Xox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HMMs():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "  # Loosely based on work from https://nlpforhackers.io/training-pos-tagger/amp/\n",
        "    cutoff = int(.75 * len(data))\n",
        "    train = data[:cutoff]\n",
        "    test = data[cutoff:]\n",
        "\n",
        "  # Original\n",
        "    uni_tag = nltk.UnigramTagger(train)\n",
        "    hmm_eval(uni_tag, test, 'Unigram')\n",
        "\n",
        "    bitag = nltk.BigramTagger(train)\n",
        "    hmm_eval(bitag, test, 'Bigram')\n",
        "\n",
        "    tritag = nltk.TrigramTagger(train)\n",
        "    hmm_eval(tritag, test, 'Trigram')"
      ],
      "metadata": {
        "id": "pWl_kFBu1no1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "ov77jdQirlo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HMMs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "yVFU5qYG2KMr",
        "outputId": "6d3889f8-408e-4f43-83d1-b098bbda73c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-02e2f079baaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHMMs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'HMMs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "ukav6W1qrnZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5QfJQ-6r-rU",
        "outputId": "74b2c233-6c77-4a7c-b0f5-ef5f8357dc47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 12812it [00:00, 74704.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pipeline] ........ (step 1 of 1) Processing classifier, total=   0.0s\n",
            "Custom calculated Accuracy:  97.15938376954355\n",
            "Precision:  75.51637279596977\n",
            "Recall:  99.33730947647449\n",
            "Fscore:  85.80423583285634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "dfQnyMBn3RVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_eval_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJslV8Ryz3R2",
        "outputId": "52f1f7bd-6417-42ef-b92a-18d6d2ad4ab4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 12812it [00:00, 147273.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401/401 [==============================] - 5s 11ms/step - loss: 0.3335 - accuracy: 0.9999\n",
            "Default Accuracy:  99.98655915260315\n",
            "Custom calculated Accuracy:  99.77364970340305\n",
            "Precision:  99.77663907502298\n",
            "Recall:  99.81598317560463\n",
            "Fscore:  99.79630724751954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_eval_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZb8n8RJI5qx",
        "outputId": "b1a85b34-5439-48d6-f574-0deab091dc7f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 1427it [00:00, 144799.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1427\n",
            "Encoding data\n",
            "Loading model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting\n",
            "Preparing predictions\n",
            "[[20 34 47 23 21 15 18 51 51 51 51 51 51 51 51 51 51 51]\n",
            " [20 13 28 13 21 15  0 34 51 51 51 51 51 51 51 51 51 51]\n",
            " [50 23  3 15  2 20 36 18 51 51 51 51 51 51 51 51 51 51]\n",
            " [34  2 34 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51]\n",
            " [18 23 47 23  3 24  2 51 51 51 51 51 51 51 51 51 51 51]]\n",
            "['seramik', 'sodomize', 'Ravinsèk', 'ene', 'karavàn']\n",
            "[('Oradye', 'BBBBBB'), ('bous', 'BBIB'), ('titan', 'BBBBI'), ('refi', 'BBBB'), ('netwayè', 'BBBBBBB')]\n",
            "Converting to graphemes\n",
            "Printing to file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently best performance is with output dim of 10, better than output dim of 50 by approx. 0.4%"
      ],
      "metadata": {
        "id": "s8x7n3OGE29_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "HQyTtVkr3SzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supervised()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn5J86L8uH9u",
        "outputId": "3dc9d0a1-683b-4b93-ed4a-51e58ada97bc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading data...: 12812it [00:00, 151729.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('k', 'B'), ('o', 'B'), ('n', 'I'), ('s', 'B'), ('i', 'B'), ('l', 'B'), ('t', 'B'), ('a', 'B'), ('n', 'I')], [('d', 'B'), ('e', 'B'), ('p', 'B'), ('o', 'B'), ('t', 'B'), ('w', 'B'), ('a', 'B')], [('s', 'B'), ('o', 'B'), ('s', 'B'), ('y', 'B'), ('o', 'B'), ('p', 'B'), ('w', 'B'), ('o', 'B'), ('f', 'B'), ('e', 'B'), ('s', 'B'), ('y', 'B'), ('o', 'B'), ('n', 'B'), ('è', 'B'), ('l', 'B')], [('v', 'B'), ('e', 'B'), ('j', 'B'), ('e', 'B'), ('t', 'B'), ('a', 'B'), ('l', 'B')], [('r', 'B'), ('e', 'B'), ('p', 'B'), ('i', 'B'), ('b', 'B'), ('l', 'B'), ('i', 'B'), ('y', 'B'), ('e', 'B')]]\n",
            "[INFO] training network...\n",
            "Epoch 1/50\n",
            "613/613 [==============================] - 44s 61ms/step - loss: 0.0758 - accuracy: 0.9585 - val_loss: 0.0086 - val_accuracy: 0.9961\n",
            "Epoch 2/50\n",
            "613/613 [==============================] - 40s 66ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0025 - val_accuracy: 0.9991\n",
            "Epoch 3/50\n",
            "613/613 [==============================] - 37s 60ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0020 - val_accuracy: 0.9991\n",
            "Epoch 4/50\n",
            "613/613 [==============================] - 37s 60ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.0013 - val_accuracy: 0.9994\n",
            "Epoch 5/50\n",
            "613/613 [==============================] - 37s 60ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 8.5848e-04 - val_accuracy: 0.9996\n",
            "Epoch 6/50\n",
            "613/613 [==============================] - 37s 61ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 6.9225e-04 - val_accuracy: 0.9998\n",
            "Epoch 7/50\n",
            "613/613 [==============================] - 37s 61ms/step - loss: 9.0674e-04 - accuracy: 0.9996 - val_loss: 5.2895e-04 - val_accuracy: 0.9998\n",
            "Epoch 8/50\n",
            "613/613 [==============================] - 39s 64ms/step - loss: 6.6649e-04 - accuracy: 0.9997 - val_loss: 4.3620e-04 - val_accuracy: 0.9998\n",
            "Epoch 9/50\n",
            "613/613 [==============================] - 37s 60ms/step - loss: 5.8740e-04 - accuracy: 0.9997 - val_loss: 3.4338e-04 - val_accuracy: 0.9998\n",
            "Epoch 10/50\n",
            "613/613 [==============================] - 37s 60ms/step - loss: 4.9880e-04 - accuracy: 0.9997 - val_loss: 2.7129e-04 - val_accuracy: 0.9999\n",
            "Epoch 11/50\n",
            "613/613 [==============================] - 38s 62ms/step - loss: 3.9811e-04 - accuracy: 0.9998 - val_loss: 2.4592e-04 - val_accuracy: 0.9999\n",
            "Epoch 12/50\n",
            "613/613 [==============================] - 38s 61ms/step - loss: 3.6715e-04 - accuracy: 0.9998 - val_loss: 3.1329e-04 - val_accuracy: 0.9998\n",
            "Epoch 13/50\n",
            "613/613 [==============================] - 37s 61ms/step - loss: 3.9022e-04 - accuracy: 0.9998 - val_loss: 4.2981e-04 - val_accuracy: 0.9998\n",
            "Epoch 14/50\n",
            "613/613 [==============================] - 39s 64ms/step - loss: 3.3528e-04 - accuracy: 0.9998 - val_loss: 2.3991e-04 - val_accuracy: 0.9999\n",
            "Epoch 15/50\n",
            "613/613 [==============================] - 38s 61ms/step - loss: 2.8985e-04 - accuracy: 0.9998 - val_loss: 1.9124e-04 - val_accuracy: 0.9999\n",
            "Epoch 16/50\n",
            "613/613 [==============================] - 37s 61ms/step - loss: 2.7631e-04 - accuracy: 0.9999 - val_loss: 2.6850e-04 - val_accuracy: 0.9999\n",
            "Epoch 17/50\n",
            "613/613 [==============================] - 37s 61ms/step - loss: 2.8309e-04 - accuracy: 0.9998 - val_loss: 2.1935e-04 - val_accuracy: 0.9998\n",
            "Epoch 18/50\n",
            "613/613 [==============================] - 37s 61ms/step - loss: 2.1236e-04 - accuracy: 0.9999 - val_loss: 1.3529e-04 - val_accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "613/613 [==============================] - 37s 61ms/step - loss: 2.2127e-04 - accuracy: 0.9999 - val_loss: 2.3146e-04 - val_accuracy: 0.9998\n",
            "Epoch 20/50\n",
            "613/613 [==============================] - 39s 64ms/step - loss: 2.1486e-04 - accuracy: 0.9998 - val_loss: 1.2752e-04 - val_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "613/613 [==============================] - 38s 62ms/step - loss: 1.6049e-04 - accuracy: 0.9999 - val_loss: 1.2601e-04 - val_accuracy: 0.9999\n",
            "Epoch 22/50\n",
            "613/613 [==============================] - 38s 62ms/step - loss: 1.5441e-04 - accuracy: 0.9999 - val_loss: 1.3940e-04 - val_accuracy: 0.9999\n",
            "Epoch 23/50\n",
            "613/613 [==============================] - 38s 63ms/step - loss: 1.6260e-04 - accuracy: 0.9999 - val_loss: 2.0564e-04 - val_accuracy: 1.0000\n",
            "41/41 [==============================] - 0s 11ms/step - loss: 8.8084e-04 - accuracy: 0.9997\n",
            "[0.0008808380807749927, 0.999696671962738]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f80695fb490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f80695f78d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom calculated Accuracy:  99.53198127925117\n",
            "Precision:  99.19354838709677\n",
            "Recall:  99.86468200270636\n",
            "Fscore:  99.52798381658799\n"
          ]
        }
      ]
    }
  ]
}