{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psriraj17/NLP-3/blob/main/semisupervised_labelSpreading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zwkkSetusdVb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "from sklearn import svm\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from numpy import concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjngr6X6sk7r",
        "outputId": "6d3d1cc7-9b10-4947-dc44-1c69c568117f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOrbZInp2lY5"
      },
      "source": [
        "Original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pEpjh6qgsoYf"
      },
      "outputs": [],
      "source": [
        "def read_file_to_sents():\n",
        "    all_data = []\n",
        "    with open(\"/content/drive/MyDrive/train_semi.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            word = line[0]\n",
        "            graphemes = line[1].split('-')\n",
        "\n",
        "            cur_word = []\n",
        "\n",
        "            for i in range(0, len(graphemes)):\n",
        "                for j in range(0, len(graphemes[i])):\n",
        "                    if j == 0:\n",
        "                        cur_word.append((graphemes[i][j], 'B'))\n",
        "                    else:\n",
        "                        cur_word.append((graphemes[i][j], 'I'))\n",
        "\n",
        "            all_data.append(cur_word)\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zD5BnvUaj3j"
      },
      "source": [
        "Original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CqSPyByEtTj9"
      },
      "outputs": [],
      "source": [
        "def split_data(data):\n",
        "    split_x = []\n",
        "    split_y = []\n",
        "    \n",
        "    for word in data:\n",
        "        cur_x = []\n",
        "        cur_y = []\n",
        "        for letter in word:\n",
        "            cur_x.append(letter[0])\n",
        "            cur_y.append(letter[1])\n",
        "\n",
        "        split_x.append(cur_x)\n",
        "        split_y.append(cur_y)\n",
        "\n",
        "    return split_x, split_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBqIXaxago5"
      },
      "source": [
        "original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NjrEIguxR2u2"
      },
      "outputs": [],
      "source": [
        "def combine_all_y(y):\n",
        "    all_y = []\n",
        "\n",
        "    for entry in y:\n",
        "        for letter in entry:\n",
        "            all_y.append(letter)\n",
        "\n",
        "    return all_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQGEbxAwaf4H"
      },
      "source": [
        "original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lfvbBNrTStwR"
      },
      "outputs": [],
      "source": [
        "def split_out(x, y):\n",
        "    fixed_y = []\n",
        "\n",
        "    track_y = 0\n",
        "\n",
        "    for j in range(0, len(x)):\n",
        "        cur_word = []\n",
        "        for i in range(0, len(x[j])):\n",
        "            cur_word.append(y[track_y])\n",
        "            track_y += 1\n",
        "        fixed_y.append(cur_word)\n",
        "\n",
        "    return fixed_y\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "yhlots7A1J2T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UlPOHdTtNt0Z"
      },
      "outputs": [],
      "source": [
        "def calc_acc(preds, y_test):\n",
        "    total_right = 0\n",
        "\n",
        "    total_size = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        for j in range(0, len(preds[i])):\n",
        "          if preds[i][j] == y_test[i][j]:\n",
        "              total_right += 1\n",
        "          total_size += 1\n",
        "\n",
        "    accuracy = 100 * (total_right/total_size)\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "-sILycj-1LH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9Si092wuN0r7"
      },
      "outputs": [],
      "source": [
        "def calc_rec(preds, y_test):\n",
        "    true_pos = 0\n",
        "    false_neg = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        for j in range(0, len(preds[i])):\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'I':\n",
        "                true_pos += 1\n",
        "            if preds[i][j] == 'B' and y_test[i][j] == 'I':\n",
        "                false_neg += 1\n",
        "\n",
        "    if true_pos + false_neg == 0:\n",
        "        return 0\n",
        "        \n",
        "    recall = 100 * (true_pos / (true_pos + false_neg))\n",
        "\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "eVGB3QZD1NOX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TWe2rpGYNxup"
      },
      "outputs": [],
      "source": [
        "def calc_prec(preds, y_test):  \n",
        "    true_pos = 0\n",
        "    false_pos = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        for j in range(0, len(preds[i])):\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'I':\n",
        "                true_pos += 1\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'B':\n",
        "                false_pos += 1\n",
        "\n",
        "    if (true_pos + false_pos) == 0:\n",
        "        return 0.01\n",
        "\n",
        "    precision = 100 * (true_pos / (true_pos + false_pos))\n",
        "\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "nrdYONiO1Omk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ISMUcoOzNnOR"
      },
      "outputs": [],
      "source": [
        "def custom_eval(preds, y_clean):\n",
        "    acc = calc_acc(preds, y_clean)\n",
        "\n",
        "    print(\"Custom calculated Accuracy: \", acc)\n",
        "\n",
        "    prec = calc_prec(preds, y_clean)\n",
        "\n",
        "    print(\"Precision: \", prec)\n",
        "\n",
        "    recall = calc_rec(preds, y_clean)\n",
        "\n",
        "    print(\"Recall: \", recall)\n",
        "\n",
        "    fscore = (2 * (prec * recall)) / (prec + recall)\n",
        "\n",
        "    print(\"Fscore: \", fscore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYgh3vOOaeHN"
      },
      "source": [
        "Based on code from https://machinelearningmastery.com/semi-supervised-learning-with-label-propagation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-NtZSkDetvsB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def semi_supervised():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    x, y = split_data(data)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5, random_state = 50)\n",
        "    clean_y_test = y_test\n",
        "    clean_x_test = x_test\n",
        "\n",
        "    x_lab = combine_all_y(x_train[:650])\n",
        "    y_lab = combine_all_y(y_train[:650])\n",
        "    x_train = combine_all_y(x_train[650:])\n",
        "\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    le.fit(y_lab)\n",
        "    y_hold = le.transform(y_lab)\n",
        "\n",
        "    x_train_mixed = concatenate((x_lab, x_train))\n",
        "    fle = LabelEncoder()\n",
        "    fle.fit(x_train_mixed)\n",
        "    x_train_mixed = fle.transform(x_train_mixed)\n",
        "    x_train_mixed = x_train_mixed.reshape(-1, 1)\n",
        "\n",
        "    nolabel = [-1 for _ in range(len(y_hold), len(x_train_mixed))]\n",
        "\n",
        "    y_train_mixed = concatenate((y_hold, nolabel))\n",
        "\n",
        "    #model = LabelPropagation(max_iter=1000, tol=0.05, n_jobs = -1, kernel='rbf', gamma=30)\n",
        "    #svc= svm.SVC(probability= True, gamma= \"auto\")\n",
        "    #model = SelfTrainingClassifier(svc)\n",
        "    model = LabelSpreading(max_iter=1000, tol=0.05, n_jobs = -1, kernel='rbf', gamma=30)\n",
        "    model.fit(x_train_mixed, y_train_mixed)\n",
        "\n",
        "    x_test = combine_all_y(x_test)\n",
        "    x_test = fle.transform(x_test)\n",
        "    x_test = x_test.reshape(-1, 1)\n",
        "\n",
        "    y_hat = model.predict(x_test)\n",
        "    y_test = combine_all_y(y_test)\n",
        "    y_test = le.transform(y_test)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "    score = accuracy_score(y_test, y_hat)\n",
        "    print(score*100)\n",
        "\n",
        "    clean_preds = le.inverse_transform(y_hat)\n",
        "    clean_preds = split_out(clean_y_test, clean_preds)\n",
        "\n",
        "    custom_eval(clean_preds, clean_y_test)\n",
        "    clean_preds = make_words(clean_preds)\n",
        "\n",
        "    clean_x_test = make_words(clean_x_test)\n",
        "\n",
        "    combined = combine_data(clean_x_test, clean_preds)\n",
        "\n",
        "    graphs = to_graphemes(combined)\n",
        "\n",
        "    print_results_to_file(graphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "ATyOolX11QV5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a0xHr2oVvLpG"
      },
      "outputs": [],
      "source": [
        "def make_words(data):\n",
        "    all_data = []\n",
        "\n",
        "    for word in data:\n",
        "        cur_word = []\n",
        "        for letter in word:\n",
        "            cur_word.append(letter)\n",
        "\n",
        "        all_data.append(''.join(cur_word))\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "Dd_J8A6a1SRM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9-M087Mewjt1"
      },
      "outputs": [],
      "source": [
        "def combine_data(x, y):\n",
        "    all_data = []\n",
        "\n",
        "    for i in range(0, len(x)):\n",
        "        all_data.append((x[i], y[i]))\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "KcU5D8af1TX1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QvZ1nhbHzoiQ"
      },
      "outputs": [],
      "source": [
        "def to_graphemes(data):\n",
        "    graph_data = []\n",
        "\n",
        "    for word_pair in data:\n",
        "        word = word_pair[0]\n",
        "        grap = word_pair[1]\n",
        "\n",
        "        cur_word = []\n",
        "\n",
        "        for i in range(0, len(word)):\n",
        "            if i == (len(word) - 1):\n",
        "                cur_word.append(word[i])\n",
        "\n",
        "            else:\n",
        "                if grap[i+1] == 'I':\n",
        "                    cur_word.append(word[i])\n",
        "                else:\n",
        "                    cur_word.append(word[i] + '-')\n",
        "\n",
        "        graph_data.append((word, ''.join(cur_word)))\n",
        "\n",
        "    return graph_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "fOTmQJ9z1Urs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results_to_file(data):\n",
        "    with open('/content/drive/MyDrive/semisupervised_results.tsv', 'w', newline='') as tsvfile:\n",
        "      writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
        "\n",
        "      for row in data:\n",
        "          writer.writerow(row)"
      ],
      "metadata": {
        "id": "gTG0TJoG1BL9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQyTtVkr3SzM"
      },
      "source": [
        "Original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn5J86L8uH9u",
        "outputId": "a9bd0845-f910-46a5-ac74-a7b3852c637c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 12812it [00:00, 55161.58it/s]\n"
          ]
        }
      ],
      "source": [
        "semi_supervised()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}