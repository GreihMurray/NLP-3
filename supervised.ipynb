{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5vXuHXp+czrz1qduTiLoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-3/blob/Super_Murray/supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zwkkSetusdVb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Reshape\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import nltk\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "-pxCyfyNrbau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2indextest.json | /content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2indextest.json PAIR WITH LOW_DIM\n",
        "#/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2indexdectree.json | /content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2indextestdectree.json PAIR WITH DEC TREE\n",
        "\n",
        "\n",
        "word2index_file = \"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/word2indextest.json\"\n",
        "tag2index_file = \"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/tag2indextest.json\""
      ],
      "metadata": {
        "id": "0kFOnQg0wQX-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjngr6X6sk7r",
        "outputId": "25444819-17c6-4d64-8df1-57f9f7eaf4ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "LOrbZInp2lY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_to_sents():\n",
        "    all_data = []\n",
        "    with open(\"/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/train.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            word = line[0]\n",
        "            graphemes = line[1].split('-')\n",
        "\n",
        "            cur_word = []\n",
        "\n",
        "            for i in range(0, len(graphemes)):\n",
        "                for j in range(0, len(graphemes[i])):\n",
        "                    if j == 0:\n",
        "                        cur_word.append((graphemes[i][j], 'B'))\n",
        "                    else:\n",
        "                        cur_word.append((graphemes[i][j], 'I'))\n",
        "\n",
        "            all_data.append(cur_word)\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "pEpjh6qgsoYf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Challenge 2 https://github.com/GreihMurray/NLP-2"
      ],
      "metadata": {
        "id": "mgV1hD6F2nLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(data):\n",
        "  vocab = list(set([w for sent in data for (w,t) in sent]))\n",
        "  vocab.append('<PAD>')\n",
        "  tags = list(set([t for sent in data for (w,t) in sent]))\n",
        "  tags.append('<PAD>')\n",
        "\n",
        "  return vocab, tags"
      ],
      "metadata": {
        "id": "SyCX8WWu4Wis"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combination of code from Challenge 2 (https://github.com/GreihMurray/NLP-2) and custom"
      ],
      "metadata": {
        "id": "KBQf1D3z2uIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(vocab, tags, data, load=False):\n",
        "  max_len = max([len(i) for i in data])\n",
        "\n",
        "  word2index = {}\n",
        "  tag2index = {}\n",
        "\n",
        "  if load is False:\n",
        "      word2index = {w: i for i, w in enumerate(vocab)}\n",
        "      tag2index = {t: i for i, t in enumerate(tags)}\n",
        "  else:\n",
        "      with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "      with open(tag2index_file) as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "  onehot = [[word2index[w[0]] for w in s] for s in data]\n",
        "  X = pad_sequences(maxlen=max_len, sequences=onehot, padding=\"post\", value=len(vocab)-1)  \n",
        "\n",
        "  onehot_y = [[tag2index[w[1]] for w in s] for s in data]\n",
        "  y = pad_sequences(maxlen=max_len, sequences=onehot_y, padding=\"post\", value=tag2index[\"<PAD>\"])\n",
        "  y = to_categorical(y, num_classes=len(tags))\n",
        "\n",
        "  # Used for saving word2index and tag2index in order to encode additional data in the same manner\n",
        "  # Currently commented out due to issues with loading model\n",
        "  with open(word2index_file, \"w\") as outfile:\n",
        "    json.dump(word2index, outfile)\n",
        "\n",
        "  with open(tag2index_file, \"w\") as outfile:\n",
        "    json.dump(tag2index, outfile)\n",
        "\n",
        "  return X, y, max_len"
      ],
      "metadata": {
        "id": "nzxivxza4ZKR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code from Challenge 2 & Dr. Scannell (https://github.com/GreihMurray/NLP-2, "
      ],
      "metadata": {
        "id": "J1ZWGtZD21XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_model(data):\n",
        "  #Original\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "  \n",
        "  \n",
        "  # Dr. Scannell\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(vocab), output_dim=10, input_length=max_len))\n",
        "    model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.05)))\n",
        "    model.add(TimeDistributed(Dense(len(tags), activation=\"softmax\")))\n",
        "    model.compile(optimizer=\"adam\", loss=\"poisson\", metrics=[\"accuracy\"])\n",
        "  # From https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "\n",
        "    print(\"[INFO] training network...\")\n",
        "    sgd = SGD(0.05)\n",
        "    history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.15, verbose=1, callbacks=stop_early)\n",
        "\n",
        "    return model, X_test, y_test"
      ],
      "metadata": {
        "id": "FVR17k8e4bxB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "68ZgdUZK3ABg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, x_test, y_test):\n",
        "    eval = model.evaluate(x_test, y_test)\n",
        "    print(eval)"
      ],
      "metadata": {
        "id": "k_MUYsLr40jx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "rZJnczhe3CiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def supervised():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    print(data[:5])\n",
        "\n",
        "    model, x_test, y_test = seq_model(data)\n",
        "\n",
        "    eval_model(model, x_test, y_test) # Eval sequential model\n",
        "\n",
        "    model.save('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32_seq_model_low_dim')\n",
        "\n",
        "    new_model = load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32_seq_model_low_dim')\n",
        "\n",
        "    eval_model(new_model, x_test, y_test)"
      ],
      "metadata": {
        "id": "-NtZSkDetvsB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "c79gNMAf3EBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_eval_model():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data, load=True)\n",
        "\n",
        "    new_model = load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/adamPoisson32_seq_model_low_dim')\n",
        "\n",
        "    evals = new_model.evaluate(x, y)\n",
        "\n",
        "    print('Default Accuracy: ', (evals[1] * 100))\n",
        "\n",
        "    preds = custom_eval(new_model, x, y)\n",
        "    clean_x = undo_encode_x(x)\n",
        "    combined = recombine(clean_x, preds)\n",
        "\n",
        "    graphs = to_graphemes(combined)\n",
        "\n",
        "    print_results_to_file(graphs)\n"
      ],
      "metadata": {
        "id": "hvLR537CzpQg"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "DVkf7ZNIredZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results_to_file(data):\n",
        "    with open('/content/gdrive/MyDrive/Colab_Notebooks/NLP/kreole/supervised_results.tsv', 'w', newline='') as tsvfile:\n",
        "      writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
        "\n",
        "      for row in data:\n",
        "          writer.writerow(row)"
      ],
      "metadata": {
        "id": "47wFNQAOqdOt"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "huJK3T_T3FtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recombine(words, preds):\n",
        "    all_data = []\n",
        "\n",
        "    for i in range(0, len(words)):\n",
        "        all_data.append((words[i], preds[i]))\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "d6R7XviOmsFj"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "v8UHkTSlrgq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_encode_x(x):\n",
        "    all_words = []\n",
        "\n",
        "    word2index = {}\n",
        "    tag2index = {}\n",
        "\n",
        "    with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "    with open(tag2index_file) as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "    for word in x:\n",
        "        cur_word = []\n",
        "        for letter in word:\n",
        "            if letter == 51:\n",
        "                break\n",
        "            true_letter = list(word2index.keys())[list(word2index.values()).index(letter)]\n",
        "            cur_word.append(true_letter)\n",
        "\n",
        "        all_words.append(''.join(cur_word))\n",
        "\n",
        "    return all_words"
      ],
      "metadata": {
        "id": "fZXYEXsZpZn2"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "NC7XKS-xriR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_encode(x, y):\n",
        "    all_data = []\n",
        "    all_words = []\n",
        "    all_tags = []\n",
        "\n",
        "    word2index = {}\n",
        "    tag2index = {}\n",
        "\n",
        "    with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "    with open(tag2index_file) as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "    for word in x:\n",
        "        cur_word = []\n",
        "        for letter in word:\n",
        "            if letter == 51:\n",
        "                break\n",
        "            true_letter = list(word2index.keys())[list(word2index.values()).index(letter)]\n",
        "            cur_word.append(true_letter)\n",
        "\n",
        "        all_words.append(''.join(cur_word))\n",
        "\n",
        "    for tags in y:\n",
        "        cur_tags = []\n",
        "        for tag in tags:\n",
        "            if tag[0] == 1:\n",
        "                cur_tags.append('I')\n",
        "            elif tag[1] == 1:\n",
        "                cur_tags.append('B')\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        all_tags.append(''.join(cur_tags))\n",
        "\n",
        "    for i in range(0, len(all_words)):\n",
        "        all_data.append((all_words[i], all_tags[i]))\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "JvMIudG_2Zv-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "Pf3yr4043Hs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_encode_y(y):\n",
        "    all_tags = []\n",
        "\n",
        "    word2index = {}\n",
        "    tag2index = {}\n",
        "\n",
        "    with open(word2index_file) as infile:\n",
        "          word2index = json.load(infile)  \n",
        "\n",
        "    with open(tag2index_file) as outfile:\n",
        "          tag2index = json.load(outfile)\n",
        "\n",
        "    for tags in y:\n",
        "        cur_tags = []\n",
        "        for tag in tags:\n",
        "            if round(tag[0]) == 1:\n",
        "                cur_tags.append('B')\n",
        "            elif round(tag[1]) == 1:\n",
        "                cur_tags.append('I')\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        all_tags.append(''.join(cur_tags))\n",
        "\n",
        "    return all_tags"
      ],
      "metadata": {
        "id": "u3ZQii5qtmgl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "akUVMppX6Y82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(preds, y_test):\n",
        "    total_right = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        if preds[i] == y_test[i]:\n",
        "            total_right += 1\n",
        "\n",
        "    accuracy = 100 * (total_right/len(preds))\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "whY7VrS66Fzz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "TLJAlt3D3J6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_precision(preds, y_test):\n",
        "    true_pos = 0\n",
        "    false_pos = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        for j in range(0, len(preds[i])):\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'I':\n",
        "                true_pos += 1\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'B':\n",
        "                false_pos += 1\n",
        "\n",
        "    if (true_pos + false_pos) == 0:\n",
        "        return 0.01\n",
        "\n",
        "    precision = 100 * (true_pos / (true_pos + false_pos))\n",
        "\n",
        "    return precision"
      ],
      "metadata": {
        "id": "Y9m8uO1d-_KH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "L47S6Rz93Ldb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_recall(preds, y_test):\n",
        "    true_pos = 0\n",
        "    false_neg = 0\n",
        "\n",
        "    for i in range(0, len(preds)):\n",
        "        for j in range(0, len(preds[i])):\n",
        "            if preds[i][j] == 'I' and y_test[i][j] == 'I':\n",
        "                true_pos += 1\n",
        "            if preds[i][j] == 'B' and y_test[i][j] == 'I':\n",
        "                false_neg += 1\n",
        "\n",
        "    if true_pos + false_neg == 0:\n",
        "        return 0\n",
        "        \n",
        "    recall = 100 * (true_pos / (true_pos + false_neg))\n",
        "\n",
        "    return recall"
      ],
      "metadata": {
        "id": "KYP5sEdp_eg7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "5q2qzQH_3NqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_eval(model, x, y):\n",
        "    preds = model.predict(x)\n",
        "\n",
        "    preds = undo_encode_y(preds)\n",
        "    y_clean = undo_encode_y(y)\n",
        "\n",
        "    acc = calc_accuracy(preds, y_clean)\n",
        "\n",
        "    print(\"Custom calculated Accuracy: \", acc)\n",
        "\n",
        "    prec = calc_precision(preds, y_clean)\n",
        "\n",
        "    print(\"Precision: \", prec)\n",
        "\n",
        "    recall = calc_recall(preds, y_clean)\n",
        "\n",
        "    print(\"Recall: \", recall)\n",
        "\n",
        "    fscore = (2 * (prec * recall)) / (prec + recall)\n",
        "\n",
        "    print(\"Fscore: \", fscore)\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "fIO2cOGUs9Ma"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "XOuh_XOC4KGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dec_tree_eval(preds, y_test):\n",
        "    acc = calc_accuracy(preds, y_test)\n",
        "\n",
        "    print(\"Custom calculated Accuracy: \", acc)\n",
        "\n",
        "    prec = calc_precision(preds, y_test)\n",
        "\n",
        "    print(\"Precision: \", prec)\n",
        "\n",
        "    recall = calc_recall(preds, y_test)\n",
        "\n",
        "    print(\"Recall: \", recall)\n",
        "\n",
        "    fscore = (2 * (prec * recall)) / (prec + recall)\n",
        "\n",
        "    print(\"Fscore: \", fscore)"
      ],
      "metadata": {
        "id": "M7Nm9ZDiyaqy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "CwJvFEFG3PvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_graphemes(data):\n",
        "    graph_data = []\n",
        "\n",
        "    for word_pair in data:\n",
        "        word = word_pair[0]\n",
        "        grap = word_pair[1]\n",
        "\n",
        "        cur_word = []\n",
        "\n",
        "        for i in range(0, len(word)):\n",
        "            if i == (len(word) - 1):\n",
        "                cur_word.append(word[i])\n",
        "\n",
        "            else:\n",
        "                if grap[i+1] == 'I':\n",
        "                    cur_word.append(word[i])\n",
        "                else:\n",
        "                    cur_word.append(word[i] + '-')\n",
        "\n",
        "        graph_data.append((word, ''.join(cur_word)))\n",
        "\n",
        "    return graph_data\n"
      ],
      "metadata": {
        "id": "Q67tirhiFCSp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "ltdgLJ9d4LrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_x_y(data):\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "\n",
        "    for word in data:\n",
        "        cur_x = []\n",
        "        cur_y = []\n",
        "        for letter in word:\n",
        "            all_x.append(letter[0])\n",
        "            all_y.append(letter[1])\n",
        "        \n",
        "\n",
        "    return np.asarray(all_x), np.asarray(all_y)"
      ],
      "metadata": {
        "id": "SX-g9foVrqyU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostly original, some work from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "Qr4wSEaB4NIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dec_tree():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    x, y = split_x_y(data)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=50)\n",
        "\n",
        "    y_test_clean = y_test\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.fit(x_train)\n",
        "    x_train = le.transform(x_train)\n",
        "    x_train = x_train.reshape(-1, 1)\n",
        "    x_test = le.transform(x_test)\n",
        "    x_test = x_test.reshape(-1, 1)\n",
        "\n",
        "    yle = LabelEncoder()\n",
        "    yle.fit(y_train)\n",
        "    y_train = yle.transform(y_train)\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_test = yle.transform(y_test)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "    clf = Pipeline([\n",
        "        ('classifier', DecisionTreeClassifier(criterion='entropy')),\n",
        "    ], verbose=1)\n",
        "\n",
        "    clf.fit(x_train, y_train)\n",
        "\n",
        "    preds = clf.predict(x_test)\n",
        "\n",
        "\n",
        "    clean_preds = yle.inverse_transform(preds)\n",
        "\n",
        "    dec_tree_eval(clean_preds, y_test_clean)"
      ],
      "metadata": {
        "id": "TL0UWmRnrm3q"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "-xDs7QF44WZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hmm_eval(model, test, model_type):\n",
        "    acc = model.accuracy(test)\n",
        "\n",
        "    print(model_type, \" Accuracy: \", acc)\n",
        "\n",
        "    prec = model.precision(test)\n",
        "\n",
        "    print(model_type, ' Precision: ', prec)\n",
        "\n",
        "    rec = model.recall(test)\n",
        "\n",
        "    print(model_type, ' Recall: ', rec)\n",
        "\n",
        "    fscore = (2 * (prec['I'] * rec['I'])) / (prec['I'] + rec['I'])\n",
        "\n",
        "    print(model_type, ' FScore (I): ', fscore)\n",
        "\n",
        "    print('\\n\\n')"
      ],
      "metadata": {
        "id": "8JFxCMl52fp5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "-MhxQZGT4Xox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HMMs():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "  # Loosely based on work from https://nlpforhackers.io/training-pos-tagger/amp/\n",
        "    cutoff = int(.75 * len(data))\n",
        "    train = data[:cutoff]\n",
        "    test = data[cutoff:]\n",
        "\n",
        "  # Original\n",
        "    uni_tag = nltk.UnigramTagger(train)\n",
        "    hmm_eval(uni_tag, test, 'Unigram')\n",
        "\n",
        "    bitag = nltk.BigramTagger(train)\n",
        "    hmm_eval(bitag, test, 'Bigram')\n",
        "\n",
        "    tritag = nltk.TrigramTagger(train)\n",
        "    hmm_eval(tritag, test, 'Trigram')"
      ],
      "metadata": {
        "id": "pWl_kFBu1no1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "ov77jdQirlo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HMMs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "yVFU5qYG2KMr",
        "outputId": "6d3889f8-408e-4f43-83d1-b098bbda73c4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-02e2f079baaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHMMs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'HMMs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "ukav6W1qrnZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5QfJQ-6r-rU",
        "outputId": "74b2c233-6c77-4a7c-b0f5-ef5f8357dc47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 12812it [00:00, 74704.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pipeline] ........ (step 1 of 1) Processing classifier, total=   0.0s\n",
            "Custom calculated Accuracy:  97.15938376954355\n",
            "Precision:  75.51637279596977\n",
            "Recall:  99.33730947647449\n",
            "Fscore:  85.80423583285634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "dfQnyMBn3RVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_eval_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJslV8Ryz3R2",
        "outputId": "fb399175-4fd9-4494-e798-c383fd2b64d5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 12812it [00:00, 157790.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401/401 [==============================] - 4s 10ms/step - loss: 0.3335 - accuracy: 0.9999\n",
            "Default Accuracy:  99.98655915260315\n",
            "Custom calculated Accuracy:  99.77364970340305\n",
            "Precision:  99.77663907502298\n",
            "Recall:  99.81598317560463\n",
            "Fscore:  99.79630724751954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently best performance is with output dim of 10, better than output dim of 50 by approx. 0.4%"
      ],
      "metadata": {
        "id": "s8x7n3OGE29_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "HQyTtVkr3SzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supervised()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn5J86L8uH9u",
        "outputId": "1094d3fe-7fab-44a6-b872-a0c5ad5f1087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 12812it [00:00, 147155.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/50\n",
            "307/307 [==============================] - 28s 73ms/step - loss: 0.4124 - accuracy: 0.9222 - val_loss: 0.3571 - val_accuracy: 0.9670\n",
            "Epoch 2/50\n",
            "307/307 [==============================] - 22s 71ms/step - loss: 0.3434 - accuracy: 0.9884 - val_loss: 0.3390 - val_accuracy: 0.9931\n",
            "Epoch 3/50\n",
            "307/307 [==============================] - 24s 78ms/step - loss: 0.3373 - accuracy: 0.9968 - val_loss: 0.3357 - val_accuracy: 0.9982\n",
            "Epoch 4/50\n",
            "307/307 [==============================] - 23s 74ms/step - loss: 0.3355 - accuracy: 0.9988 - val_loss: 0.3351 - val_accuracy: 0.9991\n",
            "Epoch 5/50\n",
            "307/307 [==============================] - 22s 72ms/step - loss: 0.3350 - accuracy: 0.9991 - val_loss: 0.3347 - val_accuracy: 0.9992\n",
            "Epoch 6/50\n",
            "307/307 [==============================] - 22s 72ms/step - loss: 0.3348 - accuracy: 0.9993 - val_loss: 0.3346 - val_accuracy: 0.9993\n",
            "Epoch 7/50\n",
            "307/307 [==============================] - 22s 73ms/step - loss: 0.3346 - accuracy: 0.9993 - val_loss: 0.3344 - val_accuracy: 0.9994\n",
            "Epoch 8/50\n",
            "307/307 [==============================] - 23s 74ms/step - loss: 0.3345 - accuracy: 0.9992 - val_loss: 0.3343 - val_accuracy: 0.9993\n",
            "Epoch 9/50\n",
            "307/307 [==============================] - 22s 73ms/step - loss: 0.3343 - accuracy: 0.9993 - val_loss: 0.3341 - val_accuracy: 0.9994\n",
            "Epoch 10/50\n",
            "307/307 [==============================] - 22s 72ms/step - loss: 0.3341 - accuracy: 0.9994 - val_loss: 0.3340 - val_accuracy: 0.9995\n",
            "Epoch 11/50\n",
            "307/307 [==============================] - 24s 78ms/step - loss: 0.3341 - accuracy: 0.9994 - val_loss: 0.3338 - val_accuracy: 0.9995\n",
            "Epoch 12/50\n",
            "307/307 [==============================] - 22s 73ms/step - loss: 0.3339 - accuracy: 0.9996 - val_loss: 0.3337 - val_accuracy: 0.9996\n",
            "Epoch 13/50\n",
            "307/307 [==============================] - 22s 73ms/step - loss: 0.3338 - accuracy: 0.9996 - val_loss: 0.3336 - val_accuracy: 0.9996\n",
            "Epoch 14/50\n",
            "307/307 [==============================] - 22s 72ms/step - loss: 0.3337 - accuracy: 0.9997 - val_loss: 0.3336 - val_accuracy: 0.9998\n",
            "Epoch 15/50\n",
            "307/307 [==============================] - 22s 71ms/step - loss: 0.3337 - accuracy: 0.9997 - val_loss: 0.3336 - val_accuracy: 0.9997\n",
            "Epoch 16/50\n",
            "307/307 [==============================] - 22s 72ms/step - loss: 0.3336 - accuracy: 0.9997 - val_loss: 0.3335 - val_accuracy: 0.9998\n",
            "Epoch 17/50\n",
            "307/307 [==============================] - 22s 72ms/step - loss: 0.3336 - accuracy: 0.9997 - val_loss: 0.3335 - val_accuracy: 0.9998\n",
            "Epoch 18/50\n",
            "307/307 [==============================] - 22s 72ms/step - loss: 0.3336 - accuracy: 0.9998 - val_loss: 0.3336 - val_accuracy: 0.9998\n",
            "Epoch 19/50\n",
            "307/307 [==============================] - 24s 79ms/step - loss: 0.3336 - accuracy: 0.9998 - val_loss: 0.3335 - val_accuracy: 0.9998\n",
            "Epoch 20/50\n",
            "307/307 [==============================] - 23s 75ms/step - loss: 0.3336 - accuracy: 0.9998 - val_loss: 0.3335 - val_accuracy: 0.9998\n",
            "Epoch 21/50\n",
            "307/307 [==============================] - 23s 74ms/step - loss: 0.3335 - accuracy: 0.9998 - val_loss: 0.3335 - val_accuracy: 0.9998\n",
            "41/41 [==============================] - 0s 11ms/step - loss: 0.3336 - accuracy: 0.9998\n",
            "[0.3335980772972107, 0.9997833371162415]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd5b9f11810> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd5b6815550> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 1s 12ms/step - loss: 0.3336 - accuracy: 0.9998\n",
            "[0.3335980772972107, 0.9997833371162415]\n"
          ]
        }
      ]
    }
  ]
}